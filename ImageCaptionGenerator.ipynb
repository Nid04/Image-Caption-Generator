{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f53fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 14:15:59.169327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e143ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd() + '/kaggle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd2c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 14:16:27.737130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756654d0343041098b965830abc41b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "    # extract feature from image\n",
    "    features = {}\n",
    "    for img_name in tqdm(os.listdir(directory)):\n",
    "        img_path = directory + '/' + img_name\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        # convert image pixels to numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # preprocess image for vgg\n",
    "        image = preprocess_input(image)\n",
    "        # extract features\n",
    "        feature = model.predict(image, verbose = 0)\n",
    "        # get image ID\n",
    "        image_id = img_name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = BASE_DIR + 'Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open(BASE_DIR + 'features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c966b858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebf9abff59941e2890baaee85d23dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loaded captions: 8092 \n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    # load the captions Data\n",
    "    file = open(os.path.join(BASE_DIR, filename), 'r')\n",
    "    captions_doc = file.read()\n",
    "    file.close()\n",
    "    return captions_doc\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "text_val = load_doc(filename)\n",
    "\n",
    "def load_captions(text_val):\n",
    "    # create mapping of image to captions\n",
    "    mapping = {}\n",
    "    # process lines\n",
    "    for line in tqdm(text_val.split('\\n')):\n",
    "        # split the line by comma(,)\n",
    "        tokens = line.split(',')\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, caption = tokens[0], tokens[1:]\n",
    "        # remove extensions from image ID\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert caption list to string\n",
    "        caption = \" \".join(caption)\n",
    "        # create list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = []\n",
    "        #store the caption\n",
    "        mapping[image_id].append(caption)\n",
    "    return mapping\n",
    "\n",
    "# parse captions\n",
    "captions = load_captions(text_val)\n",
    "print('total loaded captions: %d ' % len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491b1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_captions(captions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, cap_list in captions.items():\n",
    "        for i in range(len(cap_list)):\n",
    "            # take one caption at a time\n",
    "            captionval = cap_list[i]\n",
    "            captionval = captionval.split()\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            captionval = [word.lower() for word in captionval]\n",
    "            # replace digits, special chars, etc.,\n",
    "            # remove punctuation from each token\n",
    "            captionval = [w.translate(table) for w in captionval]\n",
    "            # remove hanging 's' and 'a'\n",
    "            captionval = [word for word in captionval if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            captionval = [word for word in captionval if word.isalpha()]\n",
    "            # store as string\n",
    "            cap_list[i] =  ' '.join(captionval)\n",
    "\n",
    "# clean captions\n",
    "clean_captions(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49531df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 2104 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91e86d3a74d4acb85406a332cbea654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covert the loaded captions into a vocabulary of words\n",
    "def to_vocab(captions):\n",
    "    all_caption = set()\n",
    "    for key in captions.keys():\n",
    "        [all_caption.update(c.split()) for c in captions[key]]\n",
    "    return all_caption\n",
    "\n",
    "print('vocabulary size: %d ' % len(to_vocab(captions)))\n",
    "\n",
    "# save captions to file, one per line\n",
    "def save_captions(captions, filename):\n",
    "    lines = []\n",
    "    file = open(os.path.join(BASE_DIR, filename), 'w')\n",
    "    for key, cap_list in tqdm(captions.items()):\n",
    "        for cap in cap_list:\n",
    "            lines.append(key + ' ' + cap)\n",
    "            data = '\\n'.join(lines)\n",
    "            file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_captions(captions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc01872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b46904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = BASE_DIR + 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(BASE_DIR + 'descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features(BASE_DIR + 'features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
